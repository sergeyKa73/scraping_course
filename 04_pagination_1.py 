import requests
import fake_useragent
from bs4 import BeautifulSoup as bs
import csv

user = fake_useragent.UserAgent().random
header = {'user-agent': user}
CSV = 'data/catalog_by.csv'


def get_html(url):
    r = requests.get(url, headers=header)
    if r.ok:  # 200  ## 403 404
        return r.text
    print(r.status_code)


def refined_cy(s):
    r = s.split(' ')[-1]  # 3200
    return r


def write_csv(data, path):
    with open(path, 'a') as file:
        writer = csv.writer(file)
        writer.writerow((data['name'],
                         data['url'],
                         data['info'],
                         data['cy']))


def get_page_content(html):
    soup = bs(html, 'lxml')
    lst_ya = soup.find_all('li', class_='yaca-snippet')

    for li in lst_ya:
        try:
            name = li.find('h2').text.strip()
        except:
            name = ''
        try:
            url = li.find('h2').find('w').get('href')
        except:
            url = ''
        try:
            info = li.find('div', class_='yaca-snippet__text').text.strip()
        except:
            info = ''
        try:
            c = li.find('div', class_="yaca-snippet__cy").text.strip()
            cy = refined_cy(c)
        except:
            cy = ''

        data = {'name': name,
                'url': url,
                'info': info,
                'cy': cy}

        write_csv(data, CSV)


def main():
    PAGENATION = input('Укажите кол-во для парсинга: ')
    PAGENATION = int(PAGENATION.strip())
    pattern = 'https://yacca.ru/geo/15/{}.html'
    for i in range(1, PAGENATION + 1):
        url = pattern.format(str(i))
        print(f'Парсинг страницы {i}')
        get_page_content(get_html(url))


if __name__ == '__main__':
    main()
